{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, Flatten, Reshape, Conv2DTranspose, Activation, BatchNormalization, LeakyReLU\n",
    "from tensorflow.keras import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-rc1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensorflow 2 버전을 사용하여 간단한 MNIST gan model 을 작성해 봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  tensorflow keras에서 제공하는 mnist 데이터셋을 가지고 옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = __\n",
    "x_train = x_train[..., tf.newaxis]\n",
    "x_test = x_test[..., tf.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 주어진 이미지는 0 ~ 255 사이의 값으로 이루어져 있습니다.\n",
    "### 학습하기 위해 이미지 픽셀 값들을 -1 ~ 1 사이의 값으로 매핑 시켜야 합니다.\n",
    "### 이 과정을 normalize 라고 합니다. \n",
    "### normalize 와 denormalize 함수를 정의하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(img):\n",
    "    \"\"\" normalize img\n",
    "    Args: \n",
    "        img: 28 * 28 size numpy array of mnist img. range(0 ~ 255)\n",
    "    Returns:\n",
    "        img: 28 * 28 size numpy array of normalized mnist img. range (-1 ~ 1)\n",
    "    \"\"\"\n",
    "    return __\n",
    "\n",
    "def denormalize(norm_img):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        norm_img : 28 * 28 size numpy array of normalized mnist img. range (-1 ~ 1)\n",
    "    Returns:\n",
    "        denormalized img: 28 * 28 size numpy array of denormalized mnist img. range (0 ~ 255)\"\"\"\n",
    "    return __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normalize 시킨 mnist 이미지들과, tensorflow dataset API를 사용하여, \n",
    "### batch size만큼 데이터를 제공하는 train, test dataset 을 정의하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = normalize(x_train), normalize(y_train)\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = __ # use tf.data.Dataset (with shuffle)\n",
    "test_dataset = __ # use tf.data.Dataset (without shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generator가 생성한 이미지와, 원래 이미지를 구분하는 discriminator를 만들어 봅시다.\n",
    "### init 함수에는 discriminator가 사용할 layer들을 정의하고, call 함수에서 정의된 layer들에 input 데이터를 통과시키세요.\n",
    "\n",
    "#### 3 * 3 커널을 512 개 가지고 있는 conv layer -> relu -> Flatten -> 256 weight dense layer -> 1 weight dense layer (logits) 를 순차적으로 통과시키세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \"\"\" define layers to use in call function\"\"\"\n",
    "        self.conv_1 = Conv2D(512, (3, 3), activation='relu')\n",
    "        self.flatten = Flatten()\n",
    "        self.dense_1 = Dense(256)\n",
    "        self.logits = Dense(1)\n",
    "        \n",
    "    def call(self, x):\n",
    "        \"\"\" let through x data into defined layers\n",
    "        Args:\n",
    "            x: image data to discriminate\n",
    "        Return:\n",
    "            logit: 1 digit logit to decide real or fake\"\"\"\n",
    "        x = __\n",
    "        x = __\n",
    "        x = __\n",
    "        x = __\n",
    "        x = __\n",
    "        return x\n",
    "    \n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss 를 계산하기 위한, cross_entropy 함수를 정의하시오.\n",
    "### loss 를 계산하기 위한, optimizer 를 정의하시오.\n",
    "\n",
    "##### 현재 구분해야할 클래스가 가짜, 진실 총 두가지 클래스 이므로, binary crossentropy 함수를 사용한다.\n",
    "##### optimizer 는 가장 보편적인 adam optimizer 를 사용한다.\n",
    "##### discriminator 와 generator 둘다 모두 따로 학습이 되므로 각각 사용할 optimizer를 정의해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가짜 이미지를 생성하기 위한, generator 를 정의해봅시다. \n",
    "#### generator 는 이미지를 생성하기 위해, NOISE 를 인풋으로 받고, 78 * 78 사이즈의 generated img 를 반환합니다.\n",
    "##### init 함수에는 generator가 사용할 layer들을 정의하고, call 함수에서 정의된 layer들에 input 데이터를 통과시키세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(Model):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        \"\"\" define layers to use in call function\"\"\"\n",
    "        self.dense_1 = Dense(7 * 7 * 1024)\n",
    "        self.reshape = Reshape((7, 7, 1024))\n",
    "        self.conv_1 = Conv2DTranspose(512, (3, 3), strides=(1, 1), padding='same')\n",
    "        self.conv_2 = Conv2DTranspose(256, (3, 3), strides=(2, 2), padding='same')\n",
    "        self.conv_3 = Conv2DTranspose(1, (3, 3), strides=(2,2), padding='same', activation='tanh')\n",
    "        \n",
    "    def call(self, latent):\n",
    "        \"\"\" let through x data into defined layers\n",
    "        Args:\n",
    "            latent: noise\n",
    "        Return:\n",
    "            generated_img: 28 * 28 generated_imgs\"\"\"\n",
    "        x = self.dense_1(latent)\n",
    "        x = self.reshape(x)\n",
    "        assert x.shape == __\n",
    "        x = self.conv_1(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU()(x)\n",
    "        assert x.shape == __\n",
    "        x = self.conv_2(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU()(x)\n",
    "        assert x.shape == __\n",
    "        x = BatchNormalization()(x)\n",
    "        x = self.conv_3(x)\n",
    "        assert x.shape == __\n",
    "        return x\n",
    "generator = Generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generator 가 제대로 작동하는지 확인해 봅시다. \n",
    "#### input 으로 100 차원의 noise 벡터를 batch_size개수 만큼 생성하고 generator를 통과해 나온 img를 그려봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x111420ac8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "latent = __ # make noise using tf.random.normal\n",
    "generated_img = # predict generated_img using generator class\n",
    "plt.imshow(generated_img[0,:,:,0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 앞서 생성한 generated_img를 정의해둔 discriminator 에 통과시켜 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision = # predict decision(logits) from generated_img using discriminator class\n",
    "assert decision.shape == __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generator를 학습시키기 위한 generator loss 함수를 정의합니다.\n",
    "#### generator가 학습하기 위해서는 discriminator 가 진짜와 generated img를 구분하지 못해야 합니다.\n",
    "#### 즉 generated img 를 discriminator가 진짜라고 간주해야 합니다.\n",
    "#### 따라서 fake_output이 진짜 (1) 로 되도록 loss 를 정의합니다. \n",
    "##### hint 앞서서 정의한 cross_entropy를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    \"\"\"define generator loss function\n",
    "    Args:\n",
    "        fake_output: logit output from discriminator(discriminator's input is generated_img)\n",
    "    Returns:\n",
    "        loss: loss value for generator\"\"\"\n",
    "    loss = cross_entropy(__, fake_output)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### discriminator 를 학습시키기 위한 discriminator loss 함수를 정의합니다.\n",
    "#### discriminator 를 학습하기 위해서는 진짜 이미지와, generated img를 잘 구분해야 합니다.\n",
    "#### 따라서 real_output은 1로, fake_output은 0 으로 다가가도록 loss 를 정의합니다.\n",
    "##### hint 앞서서 정의한 cross_entropy를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    \"\"\"define discriminator loss function\n",
    "    Args:\n",
    "        real_output: logit output from discriminator(discriminator's input is real img)\n",
    "        fake_output: logit output from discriminator(discriminator's input is generated_img)\n",
    "    Returns:\n",
    "        total_loss: loss value for discriminator \"\"\"\n",
    "    real_loss = cross_entropy(__, real_output)\n",
    "    fake_loss = cross_entropy(__, fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 이제 전체적인 학습과정을 정의해 봅시다.\n",
    "#### generator를 통해 이미지를 생성하고, 정답 이미지와, 생성된 이미지 각각 discriminator에 통과시킨 뒤, 각각의 output을 discriminator_loss 함수에 넣어 discriminator loss 를 구합니다.\n",
    "#### generator를 통해 생성된 이미지를 generator_loss 함수에 넣어 generator loss 를 구합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = __\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_imgs = __ # generate img from noise using generator class\n",
    "        fake_outputs = __ # get false output on generated_img using discriminator\n",
    "        real_outputs = __ # get real output on real_img using discriminator\n",
    "        gen_loss = __ # calculate generator loss using predefined generator_loss function\n",
    "        disc_loss = __ # calculate discriminator loss using predefined discriminator function\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "#     print(f\"g_loss == {gen_loss:4.2f} / d_loss == {disc_loss:4.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습된 generator를 사용하여 생성해 낸 이미지를 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(epoch, test_input):\n",
    "    generated_img = generator(test_input, training=False)\n",
    "    \n",
    "    fig = plt.figure(figsize=(4,4))\n",
    "    for i in range(generated_img.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(denormalized(generated_img[i, :, :, 0]), cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.savefig(f\"saved_img-epoch{epoch}.png\")\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in converted code:\n\n    <ipython-input-25-e5653a29cc10>:5 train_step  *\n        generated_imgs = generator(noise, training=True)\n    /Users/soonmok/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py:847 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    <ipython-input-8-ad0888cdd9da>:20 call  *\n        x = BatchNormalization()(x)\n    /Users/soonmok/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py:817 __call__\n        self._maybe_build(inputs)\n    /Users/soonmok/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py:2141 _maybe_build\n        self.build(input_shapes)\n    /Users/soonmok/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/normalization.py:363 build\n        experimental_autocast=False)\n    /Users/soonmok/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py:522 add_weight\n        aggregation=aggregation)\n    /Users/soonmok/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/base.py:744 _add_variable_with_custom_getter\n        **kwargs_for_getter)\n    /Users/soonmok/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py:139 make_variable\n        shape=variable_shape if variable_shape else None)\n    /Users/soonmok/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py:258 __call__\n        return cls._variable_v1_call(*args, **kwargs)\n    /Users/soonmok/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py:219 _variable_v1_call\n        shape=shape)\n    /Users/soonmok/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py:65 getter\n        return captured_getter(captured_previous, **kwargs)\n    /Users/soonmok/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py:413 invalid_creator_scope\n        \"tf.function-decorated function tried to create \"\n\n    ValueError: tf.function-decorated function tried to create variables on non-first call.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-3d69874aec2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtrain_imgs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"epoch == {epoch}... printing inference img...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1820\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1822\u001b[0;31m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1823\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgraph_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2150\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2151\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2152\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2039\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2040\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2041\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2042\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    913\u001b[0m                                           converted_func)\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    903\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 905\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    906\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in converted code:\n\n    <ipython-input-25-e5653a29cc10>:5 train_step  *\n        generated_imgs = generator(noise, training=True)\n    /Users/soonmok/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py:847 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    <ipython-input-8-ad0888cdd9da>:20 call  *\n        x = BatchNormalization()(x)\n    /Users/soonmok/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py:817 __call__\n        self._maybe_build(inputs)\n    /Users/soonmok/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py:2141 _maybe_build\n        self.build(input_shapes)\n    /Users/soonmok/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/normalization.py:363 build\n        experimental_autocast=False)\n    /Users/soonmok/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py:522 add_weight\n        aggregation=aggregation)\n    /Users/soonmok/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/base.py:744 _add_variable_with_custom_getter\n        **kwargs_for_getter)\n    /Users/soonmok/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py:139 make_variable\n        shape=variable_shape if variable_shape else None)\n    /Users/soonmok/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py:258 __call__\n        return cls._variable_v1_call(*args, **kwargs)\n    /Users/soonmok/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py:219 _variable_v1_call\n        shape=shape)\n    /Users/soonmok/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py:65 getter\n        return captured_getter(captured_previous, **kwargs)\n    /Users/soonmok/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py:413 invalid_creator_scope\n        \"tf.function-decorated function tried to create \"\n\n    ValueError: tf.function-decorated function tried to create variables on non-first call.\n"
     ]
    }
   ],
   "source": [
    "seed = tf.random.normal([4 * 4, 100])\n",
    "for epoch in range(5):\n",
    "    for train_imgs in train_dataset:\n",
    "        train_step(train_imgs)\n",
    "    print(f\"epoch == {epoch}... printing inference img...\")\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(epoch, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
